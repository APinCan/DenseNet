from load_data import load_CIFAR10
from DenseNet import DenseNet
import torch.optim as optim
from torch.optim import lr_scheduler
import torch.nn as nn
import torchvision
import torch
import matplotlib.pyplot as plt

# device = 'cuda' if torch.cuda.is_available() else 'cpu'
device = torch.device('cuda:0')


def get_lr(optimizer):
    for param_group in optimizer.param_groups:
        return param_group['lr']


def training(model, epochs, train_loader, scheduler, optimizer, criterion):
    model.train()
    x = []
    y = []

    for epoch in range(epochs):
        running_loss = 0.0
        acc = 0
        correct = 0
        total = 0
        lr = 0

        scheduler.step()
        for i, data in enumerate(train_loader):
            lr = get_lr(optimizer)

            inputs, labels = data
            inputs, labels = inputs.to(device), labels.to(device)

            optimizer.zero_grad()

            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()

            _, predicted = outputs.max(1)  # output값 중 제일 큰 값이 레이블에 가장 가까운 값
            total += labels.size(0)  # 훈련시킨 데이터의 총 개수
            correct += predicted.eq(labels).sum().item()  # correct는 맞은것의 개수

        acc = 100 * correct / total

        y.append(100 - acc)
        x.append(epoch)

        print('train epoch : {} [{}/{}]| loss: {:.3f} | acc: {:.3f} LR: {:.4f}'.format(
            epoch, i, len(train_loader), running_loss / (i + 1), acc, lr))

    print(x)
    print(y)

    torch.save(model.state_dict(), "DenseNet_CIFAR-10_100_12")


def visualize():
    x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299]
    y = [70.654, 59.588, 51.356, 46.552, 44.226, 42.41, 41.118, 40.076, 39.168, 37.934, 37.286, 36.744, 36.204, 36.076, 35.894000000000005, 35.708, 35.587999999999994, 35.714, 35.69199999999999, 35.316, 35.312, 35.022000000000006, 34.914, 34.82599999999999, 34.831999999999994, 34.55200000000001, 34.459999999999994, 35.226, 34.769999999999996, 34.482, 34.432, 34.474000000000004, 34.248000000000005, 34.096000000000004, 34.257999999999996, 34.519999999999996, 34.42, 34.17400000000001, 34.86, 34.355999999999995, 34.25, 34.141999999999996, 34.416, 34.221999999999994, 33.959999999999994, 34.226, 34.212, 33.774, 34.03, 34.092, 34.007999999999996, 34.152, 34.028000000000006, 33.82599999999999, 33.86, 33.818, 33.971999999999994, 33.620000000000005, 34.086, 33.994, 33.727999999999994, 34.05, 33.738, 33.995999999999995, 33.93600000000001, 34.111999999999995, 33.736000000000004, 33.715999999999994, 34.092, 33.79600000000001, 33.709999999999994, 33.977999999999994, 33.922, 33.873999999999995, 24.983999999999995, 22.718000000000004, 22.096000000000004, 21.915999999999997, 21.432000000000002, 21.349999999999994, 21.408, 21.147999999999996, 21.063999999999993, 20.959999999999994, 20.945999999999998, 21.067999999999998, 20.958, 20.628, 20.664, 20.641999999999996, 20.543999999999997, 20.299999999999997, 20.418000000000006, 20.197999999999993, 20.144000000000005, 20.227999999999994, 20.25, 20.058000000000007, 19.998000000000005, 19.822000000000003, 19.834000000000003, 19.879999999999995, 19.703999999999994, 19.510000000000005, 19.492000000000004, 19.266000000000005, 19.558000000000007, 19.396, 19.406000000000006, 19.495999999999995, 19.233999999999995, 19.162000000000006, 19.263999999999996, 18.849999999999994, 18.947999999999993, 19.086, 18.846000000000004, 18.831999999999994, 18.882000000000005, 18.842, 18.682000000000002, 18.689999999999998, 18.897999999999996, 18.718000000000004, 18.623999999999995, 18.67, 18.774, 18.445999999999998, 18.384, 18.215999999999994, 18.346000000000004, 18.456000000000003, 18.370000000000005, 18.394000000000005, 18.352000000000004, 18.319999999999993, 18.274, 18.391999999999996, 18.073999999999998, 18.275999999999996, 18.208, 18.519999999999996, 18.426000000000002, 18.418000000000006, 18.159999999999997, 18.567999999999998, 18.230000000000004, 18.28, 17.897999999999996, 14.081999999999994, 12.897999999999996, 12.658000000000001, 12.191999999999993, 11.947999999999993, 11.894000000000005, 11.662000000000006, 11.671999999999997, 11.482, 11.352000000000004, 11.284000000000006, 11.260000000000005, 11.123999999999995, 11.116, 10.908000000000001, 10.825999999999993, 10.983999999999995, 10.855999999999995, 10.793999999999997, 10.86, 10.799999999999997, 10.638000000000005, 10.676000000000002, 10.495999999999995, 10.456000000000003, 10.512, 10.522000000000006, 10.414000000000001, 10.388000000000005, 10.341999999999999, 10.388000000000005, 10.207999999999998, 10.358000000000004, 10.206000000000003, 10.308000000000007, 10.232, 10.164000000000001, 10.195999999999998, 10.189999999999998, 10.213999999999999, 10.227999999999994, 10.206000000000003, 10.218000000000004, 10.212000000000003, 10.219999999999999, 10.129999999999995, 9.906000000000006, 10.114000000000004, 10.207999999999998, 10.114000000000004, 9.982, 10.120000000000005, 10.043999999999997, 10.197999999999993, 9.983999999999995, 10.201999999999998, 10.126000000000005, 10.010000000000005, 10.128, 9.936000000000007, 10.091999999999999, 9.980000000000004, 10.084000000000003, 10.069999999999993, 9.995999999999995, 10.114000000000004, 10.105999999999995, 9.975999999999999, 10.102000000000004, 9.998000000000005, 10.122, 9.962000000000003, 9.995999999999995, 9.977999999999994, 10.024000000000001, 10.058000000000007, 10.058000000000007, 10.024000000000001, 10.156000000000006, 10.164000000000001, 9.936000000000007, 9.957999999999998, 9.944000000000003, 9.975999999999999, 10.158000000000001, 9.768, 10.0, 9.924000000000007, 9.977999999999994, 10.013999999999996, 9.823999999999998, 9.896, 9.906000000000006, 9.873999999999995, 9.733999999999995, 10.040000000000006, 9.945999999999998, 10.116, 9.902000000000001, 9.882000000000005, 9.902000000000001, 9.908000000000001, 9.866, 9.757999999999996, 10.134, 9.802000000000007, 9.854, 9.664000000000001, 9.686000000000007, 9.956000000000003, 9.882000000000005, 9.718000000000004, 9.715999999999994, 9.751999999999995, 9.975999999999999, 9.86, 9.587999999999994, 9.656000000000006, 9.579999999999998, 9.903999999999996, 9.584000000000003, 9.921999999999997, 9.641999999999996, 9.748000000000005, 9.772000000000006, 9.482, 9.816000000000003, 9.707999999999998, 9.781999999999996, 9.787999999999997, 9.730000000000004, 9.712000000000003, 9.691999999999993, 9.671999999999997, 9.691999999999993, 9.652000000000001, 9.665999999999997, 9.75, 9.501999999999995, 9.712000000000003, 9.707999999999998, 9.650000000000006, 9.524000000000001, 9.768, 9.605999999999995, 9.605999999999995, 9.730000000000004, 9.629999999999995, 9.561999999999998, 9.61, 9.597999999999999]

    plt.plot(x, y, label='DenseNet-100')
    plt.xlabel('epoch')
    plt.ylabel('train error')

    plt.show()

def main():
    # DenseNet의 경우 {L=40, K=12}, {L=100, K=12}, {L=100, K=24}
    # DenseNet-BC의 경우 {L=100, K=12, {L=250, K=24},  {L=190, K=40}
    print('>>> load data')
    train_loader, test_loader = load_CIFAR10()

    L = 250
    k = 24
    training_epochs = 300

    print('>>> make model')
    densenet = DenseNet(L=L, k=k)
    print(densenet)
    densenet = densenet.to(device)

    # SGD아닐지도 모름
    optimzer = optim.SGD(densenet.parameters(), weight_decay=0.001, momentum=0.9, lr=0.1, nesterov=True)
    scheduler = lr_scheduler.MultiStepLR(optimizer=optimzer,
                                         milestones=(int(training_epochs*0.5), int(training_epochs*0.25)),
                                         gamma=0.1)
    criterion = nn.CrossEntropyLoss()

    print(">>> training start")
    training(model=densenet, epochs=training_epochs, train_loader=train_loader, scheduler=scheduler,
             optimizer=optimzer, criterion=criterion)


# 시작
if __name__ == '__main__':
    main()
    # visualize()